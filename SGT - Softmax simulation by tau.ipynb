{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae26343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tau =  0.1\n",
      "1\n",
      "tau =  0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from random import choices\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#-----------------------------INTERNAL PARAMETERS----------------------------------------\n",
    "S = 3000\n",
    "T = 100\n",
    "alpha = 0.1\n",
    "epsilon = 0.3\n",
    "gamma = 0.0\n",
    "tau = np.array([0.1, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256])\n",
    "N = np.size(tau)\n",
    "l = int(T/4)\n",
    "REWARDS = np.array([-50,-5,20,10])\n",
    "THREATS = np.array([-20,-30,-100,-40])\n",
    "max_reward = np.max(REWARDS)\n",
    "max_threat = np.max(THREATS)\n",
    "#----------------------------------------------------------------------------------------\n",
    "#----------------------------------FUNCTIONS---------------------------------------------\n",
    "\n",
    "def linear_scalar(Q, w, eps):\n",
    "    temp = Q[0,:]*w[0] + Q[1,:]*w[1]\n",
    "    if np.random.uniform(0,1) < eps:\n",
    "        action = np.random.randint(0,4)\n",
    "    else:\n",
    "        action = np.random.choice(np.array(np.where(temp == np.amax(temp))).flatten())\n",
    "    return action\n",
    "\n",
    "def softmax_one (Q, w, tau):\n",
    "    temp = (Q[0,:]*w[0] + Q[1,:]*w[1])/tau \n",
    "    ex = np.exp(temp - np.max(temp))\n",
    "    weights = ex/np.sum(ex)\n",
    "    population = np.arange(np.size(Q,1))\n",
    "    action = choices(population, weights)\n",
    "    return action[0]\n",
    "\n",
    "def softmax_two (Q, w, tau):\n",
    "    ex_zero =  np.exp((Q[0,:] - np.max(Q[0,:]))/tau)\n",
    "    weights_zero = ex_zero/np.sum(ex_zero)\n",
    "    ex_one =  np.exp((Q[1,:] - np.max(Q[1,:]))/tau)\n",
    "    weights_one = ex_one/np.sum(ex_one)\n",
    "    weights = w[0]*weights_zero + w[1]*weights_one\n",
    "    population = np.arange(np.size(Q,1))\n",
    "    action = choices(population, weights)\n",
    "    return action[0]\n",
    "\n",
    "# choosing argmax over Q is put here explicitly as Amy suggested\n",
    "def update(curr_action, Q, reward, obj):\n",
    "    target = Q[obj,curr_action]\n",
    "    greedy_action = np.random.choice(np.array(np.where(Q[obj,:] == np.amax(Q[obj,:]))).flatten())\n",
    "    predict = reward + gamma*Q[obj, greedy_action]\n",
    "    Q[obj, curr_action] = (1-alpha)*target + alpha*predict\n",
    "\n",
    "def step(curr_action):\n",
    "    return REWARDS[curr_action], THREATS[curr_action]\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------TRAINING----------------------------------------------------\n",
    "p = 0.04\n",
    "W = int(1/p) + 1\n",
    "\n",
    "# recording all action sequences will be enough to recover the complete process\n",
    "# this is done to potenially save up time and space\n",
    "Q = np.zeros((S,2,4,W,N))\n",
    "actions = np.zeros((S,T,W,N))\n",
    "\n",
    "start = datetime.today()  # Get timezone naive now\n",
    "start_seconds = start.timestamp()\n",
    "\n",
    "for n in range(N):\n",
    "    print(n)\n",
    "    print(\"tau = \", tau[n])\n",
    "    for w in range(W):\n",
    "        weight = [p*w, 1.0 - p*w]\n",
    "        for s in range(S):\n",
    "            for t in range(T):\n",
    "                curr_action = softmax_one(Q[s,:,:,w,n], weight, tau[n])\n",
    "                reward, threat = step(curr_action)\n",
    "                actions[s,t,w,n] =  curr_action\n",
    "                update(curr_action, Q[s,:,:,w,n], reward, 0)\n",
    "                update(curr_action, Q[s,:,:,w,n], threat, 1)\n",
    "end = datetime.today()\n",
    "end_seconds = end.timestamp()\n",
    "print(\"execution time = \", end_seconds - start_seconds, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "regret = max_reward - REWARDS\n",
    "print(\"pure reward regrets\", regret)\n",
    "\n",
    "thr_regret = max_threat - THREATS\n",
    "print(\"pure threat regret \", thr_regret)\n",
    "\n",
    "K = 100\n",
    "reward_regret = np.zeros((S,K,W,N))\n",
    "threat_regret = np.zeros((S,K,W,N))\n",
    "for n in range(N):\n",
    "    for w in range(W):\n",
    "        for s in range(S):\n",
    "            for k in range(K):\n",
    "                a = int(actions[s,-k,w,n])\n",
    "                reward_regret[s,k,w,n] = max_reward - REWARDS[a]\n",
    "                threat_regret[s,k,w,n] = max_threat - THREATS[a]\n",
    "#------the tail average regrets to be plotted are computed below-------\n",
    "ave_reward_regret = np.mean(reward_regret, axis = (0,1))\n",
    "ave_threat_regret = np.mean(threat_regret, axis = (0,1))\n",
    "print(np.shape(ave_reward_regret))\n",
    "print(np.shape(ave_threat_regret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(20, 14), dpi=80)\n",
    "\n",
    "   \n",
    "plt.plot(thr_regret,regret, \"kx\", marker = \"o\", markersize = 5, markerfacecolor = \"black\")\n",
    "thr_regret = np.round(thr_regret, 2)\n",
    "regret = np.round(regret, 2)\n",
    "lbls = ['A ', 'B', 'C', 'D']\n",
    "font = {'family': 'serif',\n",
    "        'color':  'darkred',\n",
    "        'weight': 'normal',\n",
    "        'size': 25,\n",
    "        }\n",
    "for i in range(4):\n",
    "    plt.text(thr_regret[i], regret[i], lbls[i], fontdict = font )\n",
    "\n",
    "\n",
    "#----------------------plotting the individual policies by w and tau-------------------------------\n",
    "colors = ['red', 'green', 'blue', 'magenta', 'cyan', 'black', 'yellow', 'white']\n",
    "print(N)\n",
    "for n in range(N):\n",
    "    y = ave_reward_regret[:,n]\n",
    "   \n",
    "    x = ave_threat_regret[:,n]\n",
    "    c = np.random.randint(8)\n",
    "    plt.plot(x, y, '-')\n",
    "    plt.text(x[-1], y[-1], \"tau = \"+str(tau[n]))\n",
    "plt.xlabel(\"Threat regret\")\n",
    "plt.ylabel(\"Reward regret\")\n",
    "plt.title(\"Pareto front of Threat-Reward Regret Pairs by Simulation\" ,fontsize=15)    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
